### Surveys

- [Natural Language Processing Advancements By Deep Learning: A Survey](https://arxiv.org/abs/2003.01200)
- [Pre-trained Models for Natural Language Processing: A Survey](https://arxiv.org/abs/2003.08271)
- [Analysis Methods in Neural Language Processing: A Survey](https://arxiv.org/abs/1812.08951)
- [A Survey on Natural Language Processing for Fake News Detection](https://arxiv.org/pdf/1811.00770.pdf)
- [Recent Advances in Natural Language Inference: A Survey of Benchmarks, Resources, and Approaches](https://arxiv.org/abs/1904.01172)
- [A Review of the Neural History of Natural Language Processing](https://ruder.io/a-review-of-the-recent-history-of-nlp/)
- [A Survey of the State-of-the-Art Language Models up to Early 2020](https://medium.com/@phylypo/a-survey-of-the-state-of-the-art-language-models-up-to-early-2020-aba824302c6)
- [Trends in Integration of Vision and Language Research: A Survey of Tasks, Datasets, and Methods](https://arxiv.org/pdf/1907.09358v2.pdf)
- [Current Limitations of Language Models: What You Need is Retrieval](https://arxiv.org/pdf/2009.06857v1.pdf)

------------

- [NiuTrans/ABigSurvey](https://github.com/NiuTrans/ABigSurvey)
- [https://gluebenchmark.com/leaderboard](https://gluebenchmark.com/leaderboard)
- [https://ruder.io/](https://ruder.io/)
- [frontiers-of-natural-language-processing](https://www.slideshare.net/SebastianRuder/frontiers-of-natural-language-processing)
- [2019 — Year of BERT and Transformer](https://towardsdatascience.com/2019-year-of-bert-and-transformer-f200b53d05b9)
- [FROM Pre-trained Word Embeddings TO Pre-trained Language Models — Focus on BERT](https://towardsdatascience.com/from-pre-trained-word-embeddings-to-pre-trained-language-models-focus-on-bert-343815627598)
* [Deep Learning](http://www.nature.com/nature/journal/v521/n7553/pdf/nature14539.pdf), Nature 2015
* [LSTM: A Search Space Odyssey](http://arxiv.org/pdf/1503.04069), arXiv:1503.04069
* [A Critical Review of Recurrent Neural Networks for Sequence Learning](http://arxiv.org/pdf/1506.00019), arXiv:1506.00019
* [Visualizing and Understanding Recurrent Networks](http://arxiv.org/pdf/1506.02078), arXiv:1506.02078
* [An Empirical Exploration of Recurrent Network Architectures](http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf), ICML, 2015.
- Recent Advances in Recurrent Neural Networks. 2018. [[arXiv](https://arxiv.org/abs/1801.01078v3)]
- From Nodes to Networks: Evolving Recurrent Neural Networks. 2018. [[arXiv](https://arxiv.org/abs/1803.04439v2)]
- The History Began from AlexNet: A Comprehensive Survey on Deep Learning Approaches. 2018. [[arXiv](https://arxiv.org/abs/1803.01164v1)]
- [Natural Language Processing: State of The Art, Current Trends and Challenges](https://arxiv.org/ftp/arxiv/papers/1708/1708.05148.pdf)
- [Recent Trends in Deep Learning Based
Natural Language Processing](https://arxiv.org/pdf/1708.02709v5.pdf)
- [An Introductory Survey on Attention Mechanisms in NLP Problems](https://arxiv.org/abs/1811.05544v1)

-----------
--------------------
![natural-language-processing-nlp-tasks](https://mobidev.biz/wp-content/uploads/2019/12/natural-language-processing-nlp-tasks.png)
-----------
![B08681_01_01](https://static.packt-cdn.com/products/9781788478311/graphics/B08681_01_01.jpg)
------------
![j_dim-2020-0003_fig_009](https://content.sciendo.com/view/journals/dim/4/1/graphic/j_dim-2020-0003_fig_009.jpg)
--------------
![githubusercontent](https://camo.githubusercontent.com/144202183a5f8a47dcd0d09c7ca6df158e79d3b9/68747470733a2f2f692e7974696d672e636f6d2f76692f56387172566c65475935552f6d617872657364656661756c742e6a7067)
------------
![IdLJIaaandrB_aR_2ZCnlg](https://miro.medium.com/max/896/1*IdLJIaaandrB_aR_2ZCnlg.jpeg)
------------
![ZUHC1z5V6AsR_MOltzW](https://miro.medium.com/max/4096/1*ZUHC1z5V6AsR_MOltzW-NQ.png)
----------
![1a9030e7b256f845_articlex](https://imgs.developpaper.com/imgs/4289073911-1a9030e7b256f845_articlex.jpg)
-----------
![pretraining_adaptation](https://ruder.io/content/images/2019/08/pretraining_adaptation.png)
------------
![ner_results](https://ruder.io/content/images/2019/08/ner_results.png)
---------------
![1*ff_bprXLuTueAx7-5-MHew](https://miro.medium.com/max/1500/1*ff_bprXLuTueAx7-5-MHew.png)
--------
![FvQ12Yic_Iif2mxJB64bNw](https://miro.medium.com/max/1500/1*FvQ12Yic_Iif2mxJB64bNw.png)
-----------
![1*X1JSg2zYqD94Mp-MJRBsAw](https://miro.medium.com/max/1050/1*X1JSg2zYqD94Mp-MJRBsAw.png)
---------
![zCoB9_l5NXzlggQikrdxYg](https://miro.medium.com/max/1500/1*zCoB9_l5NXzlggQikrdxYg.png)
--------
![TurningNGL_Model__1400x788](https://www.microsoft.com/en-us/research/uploads/prod/2020/02/TurningNGL_Model__1400x788-5e418cff76a2a-1024x576.png)

-----------
![openai1](https://www.cbronline.com/wp-content/uploads/2020/06/openai1-1024x626.jpg)
----------
![in-context-prompt](https://anotherdatum.com/images//gpt-3/in-context-prompt.png)
-------------
---------------------------
