
### Reference Implementations

|| [awesome-rnn](https://github.com/kjw0612/awesome-rnn) | [char-rnn](https://github.com/karpathy/char-rnn) | [neuraltalk](https://github.com/karpathy/neuraltalk) | [neural-storyteller](https://github.com/ryankiros/neural-storyteller) | [char-rnn-tensorflow](https://github.com/sherjilozair/char-rnn-tensorflow) | [biaxial-rnn-music-composition](https://github.com/hexahedria/biaxial-rnn-music-composition) | [Element-Research/rnn](https://github.com/Element-Research/rnn) | [word-rnn-tensorflow](https://github.com/hunkim/word-rnn-tensorflow) | [recurrentjs](https://github.com/karpathy/recurrentjs) | [pytorch-qrnn](https://github.com/salesforce/pytorch-qrnn) | [rnn-tutorial-rnnlm](https://github.com/dennybritz/rnn-tutorial-rnnlm) | [crnn](https://github.com/bgshih/crnn) | [rnnlib](https://github.com/szcom/rnnlib) | [rapping-neural-network](https://github.com/robbiebarrat/rapping-neural-network) | [3D-R2N2](https://github.com/chrischoy/3D-R2N2) | [recurrentshop](https://github.com/farizrahman4u/recurrentshop) | [Dynamic-Tensorflow-Tutorial](https://github.com/KnHuq/Dynamic-Tensorflow-Tutorial) | [word-rnn](https://github.com/larspars/word-rnn) | [faster-rnnlm](https://github.com/yandex/faster-rnnlm) | [tensorflow-rnn-shakespeare](https://github.com/martin-gorner/tensorflow-rnn-shakespeare) | [theano-rnn](https://github.com/gwtaylor/theano-rnn) | [ericjang/draw](https://github.com/ericjang/draw) | [Multilabel-timeseries-classification-with-LSTM](https://github.com/aqibsaeed/Multilabel-timeseries-classification-with-LSTM) | [tensorflow-lstm-regression](https://github.com/mouradmourafiq/tensorflow-lstm-regression) | [textgenrnn](https://github.com/minimaxir/textgenrnn) | [sequence_gan](https://github.com/ofirnachum/sequence_gan) | [neuralsnap](https://github.com/rossgoodwin/neuralsnap) | [reactionrnn](https://github.com/minimaxir/reactionrnn) | [LSTM---Stock-prediction](https://github.com/jgpavez/LSTM---Stock-prediction) | [rcnn](https://github.com/taolei87/rcnn) | [rnn-nlu](https://github.com/HadoopIt/rnn-nlu) | [seq2seq-signal-prediction](https://github.com/guillaume-chevalier/seq2seq-signal-prediction) | [GRU4Rec](https://github.com/hidasib/GRU4Rec) | [keras_snli](https://github.com/Smerity/keras_snli) | [rnn_ctc](https://github.com/rakeshvar/rnn_ctc) | [BayesianRNN](https://github.com/yaringal/BayesianRNN) | [rnn-from-scratch](https://github.com/pangolulu/rnn-from-scratch) | [rnnoise](https://github.com/xiph/rnnoise) | [RNNSharp](https://github.com/zhongkaifu/RNNSharp) | [SCRNNs](https://github.com/facebookarchive/SCRNNs) | [music_rnn](https://github.com/yoavz/music_rnn) | [rnn-writer](https://github.com/robinsloan/rnn-writer) | [char-rnn-chinese](https://github.com/zhangzibin/char-rnn-chinese) | [sound-rnn](https://github.com/johnglover/sound-rnn) | [BayesianRecurrentNN](https://github.com/mirceamironenco/BayesianRecurrentNN) | [obama-rnn](https://github.com/samim23/obama-rnn) | [CRCN](https://github.com/cesc-park/CRCN) | [rnnpg](https://github.com/XingxingZhang/rnnpg) | [reseg](https://github.com/fvisin/reseg) | [Recurrent-Convolutional-Neural-Network-Text-Classifier](https://github.com/airalcorn2/Recurrent-Convolutional-Neural-Network-Text-Classifier) | [Siamese-LSTM](https://github.com/aditya1503/Siamese-LSTM) | [crypto-rnn](https://github.com/greydanus/crypto-rnn) | [tlearn-rb
](https://github.com/josephwilk/tlearn-rb) | [Recurrent-Neural-Networks](https://github.com/mohammadpz/Recurrent-Neural-Networks) | [deep-summarization](https://github.com/harpribot/deep-summarization) | [Deep-Lyrics](https://github.com/tonybeltramelli/Deep-Lyrics) | [recnet](https://github.com/joergfranke/recnet) | [tflearn_seq2seq](https://github.com/ichuang/tflearn_seq2seq) | [RNN-for-Joint-NLU](https://github.com/DSKSD/RNN-for-Joint-NLU) | [qrnn](https://github.com/DingKe/qrnn) | [Emotion-Recognition-RNN](https://github.com/saebrahimi/Emotion-Recognition-RNN) || 

|| [deepjazz](https://github.com/jisungk/deepjazz) | [torch-rnn
](https://github.com/jcjohnson/torch-rnn) | [sru](https://github.com/taolei87/sru) | [LSTM-Human-Activity-Recognition](https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition) | [tensorflow_poems](https://github.com/jinfagang/tensorflow_poems) | [DeepNLP-models-Pytorch](https://github.com/DSKSD/DeepNLP-models-Pytorch) | [tensorflow_with_latest_papers](https://github.com/NickShahML/tensorflow_with_latest_papers) | [baidu-research/persistent-rnn](https://github.com/baidu-research/persistent-rnn) | [Passage](https://github.com/IndicoDataSolutions/Passage) | [sketch-rnn](https://github.com/hardmaru/sketch-rnn) | [chatbot-rnn](https://github.com/pender/chatbot-rnn) | [pixel-rnn-tensorflow](https://github.com/carpedm20/pixel-rnn-tensorflow) | [facebook/Stack-RNN](https://github.com/facebook/Stack-RNN) | [awesome-speech-recognition-speech-synthesis-papers](https://github.com/zzw922cn/awesome-speech-recognition-speech-synthesis-papers) | [tf-rnn](https://github.com/dennybritz/tf-rnn) | [Seq2Seq-PyTorch](https://github.com/MaximumEntropy/Seq2Seq-PyTorch) | [LSTM-Sentiment-Analysis](https://github.com/adeshpande3/LSTM-Sentiment-Analysis) || 

|| [Screenshot-to-code-in-Keras](https://github.com/emilwallner/Screenshot-to-code-in-Keras) | [google/seq2seq](https://github.com/google/seq2seq) | [farizrahman4u/seq2seq](https://github.com/farizrahman4u/seq2seq) | [Conchylicultor/DeepQA](https://github.com/Conchylicultor/DeepQA) | [seq2seq-attn](https://github.com/harvardnlp/seq2seq-attn) | [Awesome-Chatbot](https://github.com/fendouai/Awesome-Chatbot) | [neuralconvo](https://github.com/macournoyer/neuralconvo) | [tensorflow-seq2seq-tutorials](https://github.com/ematvey/tensorflow-seq2seq-tutorials) | [kaggle-web-traffic](https://github.com/Arturus/kaggle-web-traffic) | [sockeye](https://github.com/awslabs/sockeye) | [IBM/pytorch-seq2seq](https://github.com/IBM/pytorch-seq2seq) | [practical_seq2seq](https://github.com/suriyadeepan/practical_seq2seq) | [Facebook-Messenger-Bot](https://github.com/adeshpande3/Facebook-Messenger-Bot) | [seq2seq-chatbot](https://github.com/tensorlayer/seq2seq-chatbot) | [seq2seq-signal-prediction](https://github.com/guillaume-chevalier/seq2seq-signal-prediction) | [tf-seq2seq](https://github.com/JayParks/tf-seq2seq) | [chatbot-retrieval](https://github.com/dennybritz/chatbot-retrieval) | [got-book-6](https://github.com/zackthoutt/got-book-6) | [LSTM-Neural-Network-for-Time-Series-Prediction](https://github.com/jaungiers/LSTM-Neural-Network-for-Time-Series-Prediction) ||


--------------------------

* basic LSTM networks(bi-directional and dynamic rnn)
* Kronecker Highway RNN
* Delta RNN
* Highway Networks
* Recurrent Highway Networks
* Multiplicative Integration Within RNNs
* Recurrent Dropout
* Layer Normalization
* Layer Normalization & Multiplicative Integration
* LSTM With Multiple Memory Arrays
* Minimal Gated Unit RNN
* Residual Connections Within Stacked RNNs
* GRU Mutants
* Weight Tying

------------------
### Surveys
* Yann LeCun, Yoshua Bengio, and Geoffrey Hinton, [Deep Learning](http://www.nature.com/nature/journal/v521/n7553/pdf/nature14539.pdf), Nature 2015
* Klaus Greff, Rupesh Kumar Srivastava, Jan Koutnik, Bas R. Steunebrink, Jurgen Schmidhuber, [LSTM: A Search Space Odyssey](http://arxiv.org/pdf/1503.04069), arXiv:1503.04069
* Zachary C. Lipton, [A Critical Review of Recurrent Neural Networks for Sequence Learning](http://arxiv.org/pdf/1506.00019), arXiv:1506.00019
* Andrej Karpathy, Justin Johnson, Li Fei-Fei, [Visualizing and Understanding Recurrent Networks](http://arxiv.org/pdf/1506.02078), arXiv:1506.02078
* Rafal Jozefowicz, Wojciech Zaremba, Ilya Sutskever, [An Empirical Exploration of Recurrent Network Architectures](http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf), ICML, 2015.


### Architectures

#### Structure

* **Bi-directional RNN [[Paper](http://www.di.ufpe.br/~fnj/RNA/bibliografia/BRNN.pdf)]**
  * *Bidirectional Recurrent Neural Networks*, Trans. on Signal Processing 1997
* **Multi-dimensional RNN [[Paper](http://arxiv.org/pdf/0705.2011.pdf)]**
  * *Multi-Dimensional Recurrent Neural Networks*, ICANN 2007
* **GFRNN [[Paper-arXiv](http://arxiv.org/pdf/1502.02367)] [[Paper-ICML](http://jmlr.org/proceedings/papers/v37/chung15.pdf)]** [[Supplementary](http://jmlr.org/proceedings/papers/v37/chung15-supp.pdf)]
  * *Gated Feedback Recurrent Neural Networks*, arXiv:1502.02367 / ICML 2015
* **Tree-Structured RNNs**
  * *Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks*, arXiv:1503.00075 / ACL 2015 [[Paper](http://arxiv.org/pdf/1503.00075)]
  * *Tree-structured composition in neural networks without tree-structured architectures*, arXiv:1506.04834 [[Paper](http://arxiv.org/pdf/1506.04834)]
* **Grid LSTM [[Paper](http://arxiv.org/pdf/1507.01526)] [[Code](https://github.com/coreylynch/grid-lstm)]**
  * *Grid Long Short-Term Memory*, arXiv:1507.01526
* **Segmental RNN [[Paper](http://arxiv.org/pdf/1511.06018v2.pdf)]**
  * "Segmental Recurrent Neural Networks", ICLR 2016.
* **Seq2seq for Sets [[Paper](http://arxiv.org/pdf/1511.06391v4.pdf)]**
  * "Order Matters: Sequence to sequence for sets", ICLR 2016.
* **Hierarchical Recurrent Neural Networks [[Paper](http://arxiv.org/abs/1609.01704)]**
  * "Hierarchical Multiscale Recurrent Neural Networks", arXiv:1609.01704

#### Memory

* **LSTM [[Paper](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf)]**
  * *Long Short-Term Memory*, Neural Computation 1997
* **GRU (Gated Recurrent Unit) [[Paper](http://arxiv.org/pdf/1406.1078.pdf)]**
  * *Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation*, arXiv:1406.1078 / EMNLP 2014
* **NTM [[Paper](http://arxiv.org/pdf/1410.5401)]**
  * *Neural Turing Machines,* arXiv preprint arXiv:1410.5401
* **Neural GPU [[Paper](http://arxiv.org/pdf/1511.08228.pdf)]**
  * arXiv:1511.08228 / ICML 2016 (under review)
* **Memory Network [[Paper](http://arxiv.org/pdf/1410.3916)]**
  * *Memory Networks,* arXiv:1410.3916
* **Pointer Network [[Paper](http://arxiv.org/pdf/1506.03134)]**
  * *Pointer Networks*, arXiv:1506.03134 / NIPS 2015
* **Deep Attention Recurrent Q-Network [[Paper](http://arxiv.org/abs/1512.01693)]**
  *  *Deep Attention Recurrent Q-Network* , arXiv:1512.01693
* **Dynamic Memory Networks [[Paper](http://arxiv.org/abs/1506.07285)]**
  * "Ask Me Anything: Dynamic Memory Networks for Natural Language Processing", arXiv:1506.07285


## Applications

### Natural Language Processing

#### Language Modeling
*  "Honza" Cernocky, Sanjeev Khudanpur, *Recurrent Neural Network based Language Model*, Interspeech 2010 [[Paper](http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf)]
* "Honza" Cernocky, Sanjeev Khudanpur, *Extensions of Recurrent Neural Network Language Model*, ICASSP 2011 [[Paper](http://www.fit.vutbr.cz/research/groups/speech/publi/2011/mikolov_icassp2011_5528.pdf)]
* *Recurrent Neural Network based Language Modeling in Meeting Recognition*, Interspeech 2011 [[Paper](http://www.fit.vutbr.cz/~imikolov/rnnlm/ApplicationOfRNNinMeetingRecognition_IS2011.pdf)]
* *A Hierarchical Neural Autoencoder for Paragraphs and Documents*, ACL 2015 [[Paper](http://arxiv.org/pdf/1506.01057)], [[Code](https://github.com/jiweil/Hierarchical-Neural-Autoencoder)]
*  *Skip-Thought Vectors*, arXiv:1506.06726 / NIPS 2015 [[Paper](http://arxiv.org/pdf/1506.06726.pdf)]
* *Character-Aware Neural Language Models*, arXiv:1508.06615 [[Paper](http://arxiv.org/pdf/1508.06615)]
* *Tree Recurrent Neural Networks with Application to Language Modeling*, arXiv:1511.00060 [[Paper](http://arxiv.org/pdf/1511.00060.pdf)]
* *The Goldilocks Principle: Reading children's books with explicit memory representations*, arXiv:1511.0230 [[Paper](http://arxiv.org/pdf/1511.02301.pdf)]


#### Speech Recognition
* *Deep Neural Networks for Acoustic Modeling in Speech Recognition*, IEEE Signam Processing Magazine 2012 [[Paper](http://cs224d.stanford.edu/papers/maas_paper.pdf)]
* *Speech Recognition with Deep Recurrent Neural Networks*, arXiv:1303.5778 / ICASSP 2013 [[Paper](http://www.cs.toronto.edu/~fritz/absps/RNN13.pdf)]
* *Attention-Based Models for Speech Recognition*, arXiv:1506.07503 / NIPS 2015 [[Paper](http://arxiv.org/pdf/1506.07503)]
* *Fast and Accurate Recurrent Neural Network Acoustic Models for Speech Recognition*, arXiv:1507.06947 2015 [[Paper](http://arxiv.org/pdf/1507.06947v1.pdf)].

#### Machine Translation
* Oxford [[Paper](http://www.nal.ai/papers/kalchbrennerblunsom_emnlp13)]
  * *Recurrent Continuous Translation Models*, EMNLP 2013
* Univ. Montreal
  * *Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation*, arXiv:1406.1078 / EMNLP 2014 [[Paper](http://arxiv.org/pdf/1406.1078)]
  * *On the Properties of Neural Machine Translation: Encoder-Decoder Approaches*, SSST-8 2014 [[Paper](http://www.aclweb.org/anthology/W14-4012)]
  * *Overcoming the Curse of Sentence Length for Neural Machine Translation using Automatic Segmentation*, SSST-8 2014
  * *Neural Machine Translation by Jointly Learning to Align and Translate*, arXiv:1409.0473 / ICLR 2015 [[Paper](http://arxiv.org/pdf/1409.0473)]
  * *On using very large target vocabulary for neural machine translation*, arXiv:1412.2007 / ACL 2015 [[Paper](http://arxiv.org/pdf/1412.2007.pdf)]
* Univ. Montreal + Middle East Tech. Univ. + Univ. Maine [[Paper](http://arxiv.org/pdf/1503.03535.pdf)]
  * *On Using Monolingual Corpora in Neural Machine Translation*, arXiv:1503.03535
* Google [[Paper](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)]
  * *Sequence to Sequence Learning with Neural Networks*, arXiv:1409.3215 / NIPS 2014
* Google + NYU [[Paper](http://arxiv.org/pdf/1410.8206)]
  * *Addressing the Rare Word Problem in Neural Machine Transltaion*, arXiv:1410.8206 / ACL 2015
* ICT + Huawei [[Paper](http://arxiv.org/pdf/1506.06442.pdf)]
  * *A Deep Memory-based Architecture for Sequence-to-Sequence Learning*, arXiv:1506.06442
* Stanford [[Paper](http://arxiv.org/pdf/1508.04025.pdf)]
  * *Effective Approaches to Attention-based Neural Machine Translation*, arXiv:1508.04025
* Middle East Tech. Univ. + NYU + Univ. Montreal [[Paper](http://arxiv.org/pdf/1601.01073.pdf)]
  * *Multi-Way, Multilingual Neural Machine Translation with a Shared Attention Mechanism*, arXiv:1601.01073

#### Conversation Modeling
* *Neural Responding Machine for Short-Text Conversation*, arXiv:1503.02364 / ACL 2015 [[Paper](http://arxiv.org/pdf/1503.02364)]
* *A Neural Conversational Model*, arXiv:1506.05869 [[Paper](http://arxiv.org/pdf/1506.05869)]
* *The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems*, arXiv:1506.08909 [[Paper](http://arxiv.org/pdf/1506.08909)]
* *Evaluating Prerequisite Qualities for Learning End-to-End Dialog Systems*, arXiv:1511.06931 [[Paper](http://arxiv.org/pdf/1511.06931)]
* *Dialog-based Language Learning*, arXiv:1604.06045, [[Paper](http://arxiv.org/pdf/1604.06045)]
* *Learning End-to-End Goal-Oriented Dialog*, arXiv:1605.07683 [[Paper](http://arxiv.org/pdf/1605.07683)]

#### Question Answering
* FAIR
  * *Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks*, arXiv:1502.05698 [[Web](https://research.facebook.com/researchers/1543934539189348)] [[Paper](http://arxiv.org/pdf/1502.05698.pdf)]
  * *Simple Question answering with Memory Networks*, arXiv:1506.02075 [[Paper](http://arxiv.org/abs/1506.02075)]
  * "The Goldilocks Principle: Reading Children's Books with Explicit Memory Representations", ICLR 2016 [[Paper](http://arxiv.org/abs/1511.02301)]
* DeepMind + Oxford [[Paper](http://arxiv.org/pdf/1506.03340.pdf)]
  * *Teaching Machines to Read and Comprehend*, arXiv:1506.03340 / NIPS 2015
* MetaMind [[Paper](http://arxiv.org/pdf/1506.07285.pdf)]
  * *Ask Me Anything: Dynamic Memory Networks for Natural Language Processing*, arXiv:1506.07285

### Computer Vision

#### Object Recognition
* *Recurrent Convolutional Neural Networks for Scene Labeling*, ICML 2014 [[Paper](http://jmlr.org/proceedings/papers/v32/pinheiro14.pdf)]
* *Recurrent Convolutional Neural Network for Object Recognition*, CVPR 2015 [[Paper](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper.pdf)]
* *Scene Labeling with LSTM Recurrent Neural Networks*, CVPR 2015 [[Paper](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Byeon_Scene_Labeling_With_2015_CVPR_paper.pdf)]
* *Recurrent Convolutional Neural Networks for Object-Class Segmentation of RGB-D Video*, IJCNN 2015 [[Paper](http://www.ais.uni-bonn.de/papers/IJCNN_2015_Pavel.pdf)]
*  *Conditional Random Fields as Recurrent Neural Networks*, arXiv:1502.03240 [[Paper](http://arxiv.org/pdf/1502.03240)]
* *Semantic Object Parsing with Local-Global Long Short-Term Memory*, arXiv:1511.04510 [[Paper](http://arxiv.org/pdf/1511.04510.pdf)]
* *Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks*, arXiv:1512.04143 / ICCV 2015 workshop [[Paper](http://arxiv.org/pdf/1512.04143)]

#### Visual Tracking
* *First Step toward Model-Free, Anonymous Object Tracking with Recurrent Neural Networks*, arXiv:1511.06425 [[Paper](http://arxiv.org/pdf/1511.06425)]


#### Image Generation
* *DRAW: A Recurrent Neural Network for Image Generation,* ICML 2015 [[Paper](http://arxiv.org/pdf/1502.04623)]
* *Unveiling the Dreams of Word Embeddings: Towards Language-Driven Image Generation,* arXiv:1506.03500 [[Paper](http://arxiv.org/pdf/1506.03500)]
* *Generative Image Modeling Using Spatial LSTMs,* arXiv:1506.03478 / NIPS 2015 [[Paper](http://arxiv.org/pdf/1506.03478)]
* *Pixel Recurrent Neural Networks,* arXiv:1601.06759 [[Paper](http://arxiv.org/abs/1601.06759)]

#### Video Analysis

* Univ. Toronto [[paper](http://arxiv.org/abs/1502.04681)]
  * *Unsupervised Learning of Video Representations using LSTMs*, arXiv:1502.04681 / ICML 2015
* Univ. Cambridge [[paper](http://arxiv.org/abs/1511.06309)]
  * *Spatio-temporal video autoencoder with differentiable memory*, arXiv:1511.06309



### Multimodal (CV + NLP)

#### Image Captioning
* UCLA + Baidu [[Web](http://www.stat.ucla.edu/~junhua.mao/m-RNN.html)] [[Paper-arXiv1](http://arxiv.org/pdf/1410.1090)], [[Paper-arXiv2](http://arxiv.org/pdf/1412.6632)]
  * Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, and Alan L. Yuille, *Explain Images with Multimodal Recurrent Neural Networks*, arXiv:1410.1090
  * Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Zhiheng Huang, and Alan L. Yuille, *Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)*, arXiv:1412.6632 / ICLR 2015
* Univ. Toronto [[Paper](http://arxiv.org/pdf/1411.2539)] [[Web demo](http://deeplearning.cs.toronto.edu/i2t)]
  * Ryan Kiros, Ruslan Salakhutdinov, and Richard S. Zemel, *Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models*, arXiv:1411.2539 / TACL 2015
* Berkeley [[Web](http://jeffdonahue.com/lrcn/)] [[Paper](http://arxiv.org/pdf/1411.4389)]
  * Jeff Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, and Trevor Darrell, *Long-term Recurrent Convolutional Networks for Visual Recognition and Description*, arXiv:1411.4389 / CVPR 2015
* Google [[Paper](http://arxiv.org/pdf/1411.4555)]
  * Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan, *Show and Tell: A Neural Image Caption Generator*, arXiv:1411.4555 / CVPR 2015
* Stanford [[Web]](http://cs.stanford.edu/people/karpathy/deepimagesent/) [[Paper]](http://cs.stanford.edu/people/karpathy/cvpr2015.pdf)
  * Andrej Karpathy and Li Fei-Fei, *Deep Visual-Semantic Alignments for Generating Image Description*, CVPR 2015
* Microsoft [[Paper](http://arxiv.org/pdf/1411.4952)]
  * Hao Fang, Saurabh Gupta, Forrest Iandola, Rupesh Srivastava, Li Deng, Piotr Dollar, Jianfeng Gao, Xiaodong He, Margaret Mitchell, John C. Platt, Lawrence Zitnick, and Geoffrey Zweig, *From Captions to Visual Concepts and Back*, arXiv:1411.4952 / CVPR 2015
* CMU + Microsoft [[Paper-arXiv](http://arxiv.org/pdf/1411.5654)], [[Paper-CVPR](http://www.cs.cmu.edu/~xinleic/papers/cvpr15_rnn.pdf)]
  * Xinlei Chen, and C. Lawrence Zitnick, *Learning a Recurrent Visual Representation for Image Caption Generation*
  * Xinlei Chen, and C. Lawrence Zitnick, *Mind’s Eye: A Recurrent Visual Representation for Image Caption Generation*, CVPR 2015
* Univ. Montreal + Univ. Toronto [[Web](http://kelvinxu.github.io/projects/capgen.html)] [[Paper](http://www.cs.toronto.edu/~zemel/documents/captionAttn.pdf)]
  * Kelvin Xu, Jimmy Lei Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard S. Zemel, and Yoshua Bengio, *Show, Attend, and Tell: Neural Image Caption Generation with Visual Attention*, arXiv:1502.03044 / ICML 2015
* Idiap + EPFL + Facebook [[Paper](http://arxiv.org/pdf/1502.03671)]
  * Remi Lebret, Pedro O. Pinheiro, and Ronan Collobert, *Phrase-based Image Captioning*, arXiv:1502.03671 / ICML 2015
* UCLA + Baidu [[Paper](http://arxiv.org/pdf/1504.06692)]
  * Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Zhiheng Huang, and Alan L. Yuille, *Learning like a Child: Fast Novel Visual Concept Learning from Sentence Descriptions of Images*, arXiv:1504.06692
* MS + Berkeley
  * Jacob Devlin, Saurabh Gupta, Ross Girshick, Margaret Mitchell, and C. Lawrence Zitnick, *Exploring Nearest Neighbor Approaches for Image Captioning*, arXiv:1505.04467 (Note: technically not RNN) [[Paper](http://arxiv.org/pdf/1505.04467.pdf)]
  * Jacob Devlin, Hao Cheng, Hao Fang, Saurabh Gupta, Li Deng, Xiaodong He, Geoffrey Zweig, and Margaret Mitchell, *Language Models for Image Captioning: The Quirks and What Works*, arXiv:1505.01809 [[Paper](http://arxiv.org/pdf/1505.01809.pdf)]
* Adelaide [[Paper](http://arxiv.org/pdf/1506.01144.pdf)]
  * Qi Wu, Chunhua Shen, Anton van den Hengel, Lingqiao Liu, and Anthony Dick, *Image Captioning with an Intermediate Attributes Layer*, arXiv:1506.01144
* Tilburg [[Paper](http://arxiv.org/pdf/1506.03694.pdf)]
  * Grzegorz Chrupala, Akos Kadar, and Afra Alishahi, *Learning language through pictures*, arXiv:1506.03694
* Univ. Montreal [[Paper](http://arxiv.org/pdf/1507.01053.pdf)]
  * Kyunghyun Cho, Aaron Courville, and Yoshua Bengio, *Describing Multimedia Content using Attention-based Encoder-Decoder Networks*, arXiv:1507.01053
* Cornell [[Paper](http://arxiv.org/pdf/1508.02091.pdf)]
  * Jack Hessel, Nicolas Savva, and Michael J. Wilber, *Image Representations and New Domains in Neural Image Captioning*, arXiv:1508.02091


#### Video Captioning
* Berkeley [[Web](http://jeffdonahue.com/lrcn/)] [[Paper](http://arxiv.org/pdf/1411.4389)]
  *  *Long-term Recurrent Convolutional Networks for Visual Recognition and Description*, arXiv:1411.4389 / CVPR 2015
* UT Austin + UML + Berkeley [[Paper](http://arxiv.org/pdf/1412.4729)]
  * *Translating Videos to Natural Language Using Deep Recurrent Neural Networks*, arXiv:1412.4729
* Microsoft [[Paper](http://arxiv.org/pdf/1505.01861)]
  * *Joint Modeling Embedding and Translation to Bridge Video and Language*, arXiv:1505.01861
* UT Austin + Berkeley + UML [[Paper](http://arxiv.org/pdf/1505.00487)]
  * *Sequence to Sequence--Video to Text*, arXiv:1505.00487
* Univ. Montreal + Univ. Sherbrooke [[Paper](http://arxiv.org/pdf/1502.08029.pdf)]
  * *Describing Videos by Exploiting Temporal Structure*, arXiv:1502.08029
* MPI + Berkeley [[Paper](http://arxiv.org/pdf/1506.01698.pdf)]
  * *The Long-Short Story of Movie Description*, arXiv:1506.01698
* Univ. Toronto + MIT [[Paper](http://arxiv.org/pdf/1506.06724.pdf)]
  * *Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books*, arXiv:1506.06724
* Univ. Montreal [[Paper](http://arxiv.org/pdf/1507.01053.pdf)]
  * *Describing Multimedia Content using Attention-based Encoder-Decoder Networks*, arXiv:1507.01053
* Zhejiang Univ. + UTS [[Paper](http://arxiv.org/abs/1511.03476)]
  * *Hierarchical Recurrent Neural Encoder for Video Representation with Application to Captioning*, arXiv:1511.03476
* Univ. Montreal + NYU + IBM [[Paper](http://arxiv.org/pdf/1511.04590.pdf)]
  * *Empirical performance upper bounds for image and video captioning*, arXiv:1511.04590


#### Visual Question Answering

* Virginia Tech. + MSR [[Web](http://www.visualqa.org/)] [[Paper](http://arxiv.org/pdf/1505.00468)]
  * *VQA: Visual Question Answering*, arXiv:1505.00468 / CVPR 2015 SUNw:Scene Understanding workshop
* MPI + Berkeley [[Web](https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/vision-and-language/visual-turing-challenge/)] [[Paper](http://arxiv.org/pdf/1505.01121)]
  * *Ask Your Neurons: A Neural-based Approach to Answering Questions about Images*, arXiv:1505.01121
* Univ. Toronto [[Paper](http://arxiv.org/pdf/1505.02074)] [[Dataset](http://www.cs.toronto.edu/~mren/imageqa/data/cocoqa/)]
  * *Exploring Models and Data for Image Question Answering*, arXiv:1505.02074 / ICML 2015 deep learning workshop
* Baidu + UCLA [[Paper](http://arxiv.org/pdf/1505.05612)] [[Dataset]()]
  * *Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question Answering*, arXiv:1505.05612 / NIPS 2015
* SNU + NAVER [[Paper](http://arxiv.org/abs/1606.01455)]
  * *Multimodal Residual Learning for Visual QA*, arXiv:1606:01455
* UC Berkeley + Sony [[Paper](https://arxiv.org/pdf/1606.01847)]
  * *Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding*, arXiv:1606.01847
* Postech [[Paper](http://arxiv.org/pdf/1606.03647.pdf)]
  * *Training Recurrent Answering Units with Joint Loss Minimization for VQA*, arXiv:1606.03647
* SNU + NAVER [[Paper](http://arxiv.org/abs/1610.04325)]
  * *Hadamard Product for Low-rank Bilinear Pooling*, arXiv:1610.04325
* Video QA
  * CMU + UTS [[paper](http://arxiv.org/abs/1511.04670)]
    * Uncovering Temporal Context for Video Question and Answering, arXiv:1511.04670
  * KIT + MIT + Univ. Toronto [[Paper](http://arxiv.org/abs/1512.02902)] [[Dataset](http://movieqa.cs.toronto.edu/home/)]
    * MovieQA: Understanding Stories in Movies through Question-Answering, arXiv:1512.02902


#### Turing Machines
* *Neural Turing Machines,* arXiv preprint arXiv:1410.5401 [[Paper](http://arxiv.org/pdf/1410.5401)]
* *Memory Networks,* arXiv:1410.3916 [[Paper](http://arxiv.org/pdf/1410.3916)]
* *Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets*, arXiv:1503.01007 / NIPS 2015 [[Paper](http://arxiv.org/pdf/1503.01007)]
* *End-To-End Memory Networks*, arXiv:1503.08895 / NIPS 2015 [[Paper](http://arxiv.org/pdf/1503.08895)]
* *Reinforcement Learning Neural Turing Machines,* arXiv:1505.00521 [[Paper](http://arxiv.org/pdf/1505.00521)]
* *Recurrent Neural Networks with External Memory for Language Understanding*, arXiv:1506.00195 [[Paper](http://arxiv.org/pdf/1506.00195.pdf)]
* *A Deep Memory-based Architecture for Sequence-to-Sequence Learning*, arXiv:1506.06442 [[Paper](http://arxiv.org/pdf/1506.06442.pdf)]
* A*Neural Programmer: Inducing Latent Programs with Gradient Descent*, arXiv:1511.04834 [[Paper](http://arxiv.org/pdf/1511.04834.pdf)]
* *Neural Programmer-Interpreters*, arXiv:1511.06279 [[Paper](http://arxiv.org/pdf/1511.06279.pdf)]
* *Neural Random-Access Machines*, arXiv:1511.06392 [[Paper](http://arxiv.org/pdf/1511.06392.pdf)]
* Łukasz Kaiser and Ilya Sutskever, *Neural GPUs Learn Algorithms*, arXiv:1511.08228 [[Paper](http://arxiv.org/pdf/1511.08228.pdf)]
* *Skip-Thought Memory Networks*, arXiv:1511.6420 [[Paper](https://pdfs.semanticscholar.org/6b9f/0d695df0ce01d005eb5aa69386cb5fbac62a.pdf)]
* *Learning Simple Algorithms from Examples*, arXiv:1511.07275 [[Paper](http://arxiv.org/pdf/1511.07275.pdf)]

### Robotics

* *Listen, Attend, and Walk: Neural Mapping of Navigational Instructions to Action Sequences*, arXiv:1506.04089 [[Paper](http://arxiv.org/pdf/1506.04089.pdf)]
* *Policy Learning with Continuous Memory States for Partially Observed Robotic Control,* arXiv:1507.01273. [[Paper]](http://arxiv.org/pdf/1507.01273)

### Other
* *Generating Sequences With Recurrent Neural Networks,* arXiv:1308.0850 [[Paper]](http://arxiv.org/abs/1308.0850)
* *Recurrent Models of Visual Attention*, NIPS 2014 / arXiv:1406.6247 [[Paper](http://arxiv.org/pdf/1406.6247.pdf)]
* *Learning to Execute*, arXiv:1410.4615 [[Paper](http://arxiv.org/pdf/1410.4615.pdf)] [[Code](https://github.com/wojciechz/learning_to_execute)]
* *Scheduled Sampling for Sequence Prediction with
Recurrent Neural Networks*, arXiv:1506.03099 / NIPS 2015 [[Paper](http://arxiv.org/pdf/1506.03099)]
* *DAG-Recurrent Neural Networks For Scene Labeling*, arXiv:1509.00552 [[Paper](http://arxiv.org/pdf/1509.00552)]
* *Recurrent Spatial Transformer Networks*, arXiv:1509.05329 [[Paper](http://arxiv.org/pdf/1509.05329)]
* *Batch Normalized Recurrent Neural Networks*, arXiv:1510.01378 [[Paper](http://arxiv.org/pdf/1510.01378)]
* *Deeply-Recursive Convolutional Network for Image Super-Resolution*, arXiv:1511.04491 [[Paper]](http://arxiv.org/abs/1511.04491)
* *First Step toward Model-Free, Anonymous Object Tracking with Recurrent Neural Networks*, arXiv:1511.06425 [[Paper](http://arxiv.org/pdf/1511.06425.pdf)]
* *ReSeg: A Recurrent Neural Network for Object Segmentation*, arXiv:1511.07053 [[Paper](http://arxiv.org/pdf/1511.07053.pdf)]
* *On Learning to Think: Algorithmic Information Theory for Novel Combinations of Reinforcement Learning Controllers and Recurrent Neural World Models*, arXiv:1511.09249 [[Paper]](http://arxiv.org/pdf/1511.09249)


------------------------


----------------------
